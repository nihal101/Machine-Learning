# -*- coding: utf-8 -*-
"""Copy of data_preprocessing_tools.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fzo93MQ-wl103U9a9ktF4pVVu8UtJcZI

# Data Preprocessing Tools

## Importing the libraries

We have to import these three library for data preprocessing.
1.numpy-:numpy stands for (Numerical Python) and it is a library for working with arrays.

2.pandas-:pandas is an open source library that is built on top of numpy library.It is python package that provide various data structure and operation for manuipulating numerical. It is also used to import data set.

3.matplotlib-: matplotlib is a python library used for 2D graphics in python
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

"""## Importing the dataset

In first line we call method "read_csv" which present in pandas and it takes argument that is path of dataset.

X is known as design matrix which contain matrix of input feature and it can formed by taking all coloumn except last(because last column is outfeature)

y contain last column of out dataset.
"""

dataset=pd.read_csv('Data.csv')
X=dataset.iloc[:,:-1].values
y=dataset.iloc[:,-1].values

"""This is how design matrix X looks like."""

print(X)

"""This is how output feature(last column) looks like"""

print(y)

"""## Taking care of missing data

In design matrix X we can some see some field where are written "nan" which means null values. So before going to apply out machine learning model on dataset we have to fill thsese null field.We can do that in different approach.

I-:fill with mean value.
II-:fill with median value.
III-:fill with most frequent element in X.

Here I am going to 'mean' strategy
"""

from sklearn.impute import SimpleImputer
imputer=SimpleImputer(missing_values=np.nan,strategy='mean')
imputer.fit(X[:,1:3])
X[:,1:3]=imputer.transform(X[:,1:3])

"""After filling null field with mean value our design matrix will be changed and looks like as below"""

print(X)

"""## Encoding categorical data

### Encoding the Independent Variable

It may be out data set may contain some feature(column) in categorical fashion.(In out data set country name and purchased column contain categorical data). So we have to encode it in numerical Type. There are three ways for that.

I-:Integer Coding ("Where each unique label is mapped to an integer")

II-:One Hot Encoding ("Where each label is mapped to binary number")

III-:Learned Embedding 

Here I am using One Hot Encoding
"""

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
ct = ColumnTransformer(transformers =[('encode',OneHotEncoder(),[0])] , remainder = 'passthrough')
X=np.array(ct.fit_transform(X))

"""After applying One Hot Encoding out design matrix will be completely changed."""

print(X)

"""### Encoding the Dependent Variable

Like design matrix we have also apply One Hot Encoding for our output feature it it contain any categorical data
"""

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y = le.fit_transform(y)

"""Our new output feature after One Hot Encoding."""

print(y)

"""## Splitting the dataset into the Training set and Test set

We have to divide out data set into two part. first part that is called Training Set and it is used to train out model. Second part is called test_test which used to test the model how accurately predict output.
"""

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=1)

"""It print Training data set."""

print(X_train)

print(y_train)

"""It print Test data set."""

print(X_test)

print(y_test)

"""## Feature Scaling

Feature Scalling is an important step of data preprocessing. Feature Scaling makes all data in such way that they lie in same scale usually -3 to +3.

In out data set some field have small value and some field have large value. If we apply out machine learning model without feature scaling then prediction our model have high cost(It does because small value are dominated by large value). 
So before apply model we have to perform feature scaling.

We can perform feature scaling in two ways.

I-:Standardizaion
    x=(x-mean(X))/standard deviation(X)
II-:Normalization-:
    x=(x-min(X))/(max(X)-min(X))
"""

from sklearn.preprocessing import StandardScaler
sc =  StandardScaler()
X_train[:,3:] = sc.fit_transform(X_train[:,3:])
X_test[:,3:] = sc.transform(X_test[:,3:])

print(X_train)

print(X_test)